name: NanaSQLite Tests

on:
  push:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ${{ (github.ref == 'refs/heads/staging' || github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' || github.event_name == 'pull_request') && fromJson('["3.9", "3.10", "3.11", "3.12", "3.13", "3.14"]') || fromJson('["3.12", "3.13"]') }}

    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        uv pip install --system pytest pytest-benchmark pytest-asyncio pytest-xdist apsw pydantic
        uv pip install --system -e .
    
    - name: Run tests
      run: |
        pytest tests/ -v --tb=short --junitxml=test-results.xml -n auto --ignore=tests/test_benchmark.py --ignore=tests/test_async_benchmark.py
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: test-results.xml

  benchmark:
    runs-on: ${{ matrix.os }}
    needs: test
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ${{ (github.ref == 'refs/heads/staging' || github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' || github.event_name == 'pull_request') && fromJson('["3.9", "3.10", "3.11", "3.12", "3.13", "3.14"]') || fromJson('["3.12", "3.13"]') }}

    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        uv pip install --system pytest pytest-benchmark pytest-asyncio apsw pydantic
        uv pip install --system -e .
    
    - name: Run sync benchmarks
      run: |
        pytest tests/test_benchmark.py -v --benchmark-only --benchmark-json=benchmark.json || true
    
    - name: Upload sync benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: benchmark.json

  async-benchmark:
    runs-on: ${{ matrix.os }}
    needs: test
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ${{ (github.ref == 'refs/heads/staging' || github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' || github.event_name == 'pull_request') && fromJson('["3.9", "3.10", "3.11", "3.12", "3.13", "3.14"]') || fromJson('["3.12", "3.13"]') }}

    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        uv pip install --system pytest pytest-benchmark pytest-asyncio apsw pydantic
        uv pip install --system -e .
    
    - name: Run async benchmarks
      run: |
        pytest tests/test_async_benchmark.py -v --benchmark-only --benchmark-json=async-benchmark.json || true
    
    - name: Upload async benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: async-benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: async-benchmark.json

  benchmark-summary:
    needs: [benchmark, async-benchmark]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download sync benchmark results
      uses: actions/download-artifact@v7
      with:
        pattern: benchmark-results-*
        path: benchmark-results
    
    - name: Download async benchmark results
      uses: actions/download-artifact@v7
      with:
        pattern: async-benchmark-results-*
        path: async-benchmark-results
    
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'
    
    - name: Generate Combined Benchmark Summary
      run: |
        echo "## ðŸ“Š Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ”„ Sync Benchmarks (NanaSQLite)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        python3 scripts/parse_benchmark.py benchmark-results sync >> $GITHUB_STEP_SUMMARY || echo "Failed to parse sync benchmark results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### âš¡ Async Benchmarks (AsyncNanaSQLite)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        python3 scripts/parse_benchmark.py async-benchmark-results async >> $GITHUB_STEP_SUMMARY || echo "Failed to parse async benchmark results" >> $GITHUB_STEP_SUMMARY

  test-summary:
    needs: [test, benchmark-summary]
    runs-on: ubuntu-latest
    if: always()
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v7
      with:
        pattern: test-results-*
        path: test-results
    
    - name: Test Summary
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| OS | Python | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|---|---|---|" >> $GITHUB_STEP_SUMMARY
        for dir in test-results/*/; do
          name=$(basename "$dir")
          os=$(echo "$name" | sed 's/test-results-\(.*\)-py.*/\1/')
          py=$(echo "$name" | sed 's/.*-py\(.*\)/\1/')
          if [ -f "$dir/test-results.xml" ]; then
            errors=$(grep -o 'errors="[0-9]*"' "$dir/test-results.xml" | head -1 | grep -o '[0-9]*')
            failures=$(grep -o 'failures="[0-9]*"' "$dir/test-results.xml" | head -1 | grep -o '[0-9]*')
            if [ "$errors" = "0" ] && [ "$failures" = "0" ]; then
              echo "| $os | $py | âœ… Passed |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| $os | $py | âŒ Failed |" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "| $os | $py | âš ï¸ No results |" >> $GITHUB_STEP_SUMMARY
          fi
        done
