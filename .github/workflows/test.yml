name: NanaSQLite Tests

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.10', '3.11', '3.12', '3.13']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark apsw
        pip install -e .
    
    - name: Run tests
      run: |
        pytest tests/test_nanasqlite.py -v --tb=short --junitxml=test-results.xml
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: test-results.xml

  benchmark:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark apsw
        pip install -e .
    
    - name: Run benchmarks
      run: |
        pytest tests/test_benchmark.py -v --benchmark-only --benchmark-json=benchmark.json || true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark.json
    
    - name: Benchmark Summary
      run: |
        echo "## ðŸ“Š Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [ -f benchmark.json ]; then
          echo "| Test | Mean | Rounds |" >> $GITHUB_STEP_SUMMARY
          echo "|------|------|--------|" >> $GITHUB_STEP_SUMMARY
          python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
        import json
        with open('benchmark.json') as f:
            data = json.load(f)
        for bench in data.get('benchmarks', []):
            name = bench['name'].split('::')[-1]
            mean = bench['stats']['mean'] * 1000
            rounds = bench['stats']['rounds']
            print(f'| {name} | {mean:.3f}ms | {rounds} |')
        EOF
        else
          echo "ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯çµæžœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" >> $GITHUB_STEP_SUMMARY
        fi

  test-summary:
    needs: [test, benchmark]
    runs-on: ubuntu-latest
    if: always()
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v6
      with:
        pattern: test-results-*
        path: test-results
    
    - name: Test Summary
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| OS | Python | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|---|---|---|" >> $GITHUB_STEP_SUMMARY
        for dir in test-results/*/; do
          name=$(basename "$dir")
          os=$(echo "$name" | sed 's/test-results-\(.*\)-py.*/\1/')
          py=$(echo "$name" | sed 's/.*-py\(.*\)/\1/')
          if [ -f "$dir/test-results.xml" ]; then
            errors=$(grep -o 'errors="[0-9]*"' "$dir/test-results.xml" | head -1 | grep -o '[0-9]*')
            failures=$(grep -o 'failures="[0-9]*"' "$dir/test-results.xml" | head -1 | grep -o '[0-9]*')
            if [ "$errors" = "0" ] && [ "$failures" = "0" ]; then
              echo "| $os | $py | âœ… Passed |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| $os | $py | âŒ Failed |" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "| $os | $py | âš ï¸ No results |" >> $GITHUB_STEP_SUMMARY
          fi
        done
