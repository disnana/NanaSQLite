name: NanaSQLite Benchmarks

on:
  push:
    branches:
      - main
    paths:
      - 'src/**'
      - 'scripts/parse_benchmark.py'
      - '.github/workflows/benchmark.yml'
  pull_request:
    paths:
      - 'src/**'
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: write
  deployments: write

jobs:
  benchmark:
    runs-on: ${{ matrix.os }}
    timeout-minutes: 15
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.11", "3.13"]
        exclude:
          # Windows: only 3.13
          - os: windows-latest
            python-version: "3.11"

    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Setup Windows temp directory
      if: runner.os == 'Windows'
      shell: powershell
      run: New-Item -ItemType Directory -Force -Path D:\temp

    - name: Install dependencies
      run: |
        uv pip install --system pytest pytest-benchmark pytest-asyncio apsw pydantic
        uv pip install --system -e .
    
    - name: Run sync benchmarks (Windows)
      if: runner.os == 'Windows'
      env:
        TEMP: 'D:\temp'
        TMP: 'D:\temp'
      run: |
        pytest tests/test_benchmark.py -v --benchmark-only --benchmark-json=benchmark.json || true

    - name: Run sync benchmarks (Linux/macOS)
      if: runner.os != 'Windows'
      run: |
        pytest tests/test_benchmark.py -v --benchmark-only --benchmark-json=benchmark.json || true
    
    - name: Upload sync benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: benchmark.json
        retention-days: 7

  async-benchmark:
    runs-on: ${{ matrix.os }}
    timeout-minutes: 15
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.11", "3.13"]
        exclude:
          # Windows: only 3.13
          - os: windows-latest
            python-version: "3.11"

    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Setup Windows temp directory
      if: runner.os == 'Windows'
      shell: powershell
      run: New-Item -ItemType Directory -Force -Path D:\temp

    - name: Install dependencies
      run: |
        uv pip install --system pytest pytest-benchmark pytest-asyncio apsw pydantic
        uv pip install --system -e .
    
    - name: Run async benchmarks (Windows)
      if: runner.os == 'Windows'
      env:
        TEMP: 'D:\temp'
        TMP: 'D:\temp'
      run: |
        pytest tests/test_async_benchmark.py -v --benchmark-only --benchmark-json=async-benchmark.json || true

    - name: Run async benchmarks (Linux/macOS)
      if: runner.os != 'Windows'
      run: |
        pytest tests/test_async_benchmark.py -v --benchmark-only --benchmark-json=async-benchmark.json || true
    
    - name: Upload async benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: async-benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: async-benchmark.json
        retention-days: 7

  benchmark-summary:
    needs: [benchmark, async-benchmark]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    if: always()
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Fetch gh-pages branch
      if: github.ref == 'refs/heads/main'
      run: |
        git fetch origin gh-pages:gh-pages || echo "gh-pages branch not found, will be created"
    
    - name: Download sync benchmark results
      uses: actions/download-artifact@v4
      with:
        pattern: benchmark-results-*
        path: benchmark-results
    
    - name: Download async benchmark results
      uses: actions/download-artifact@v4
      with:
        pattern: async-benchmark-results-*
        path: async-benchmark-results
    
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'
    
    - name: Generate Combined Benchmark Summary
      run: |
        echo "## ðŸ“Š Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ”„ Sync Benchmarks (NanaSQLite)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        python3 scripts/parse_benchmark.py benchmark-results sync >> $GITHUB_STEP_SUMMARY || echo "Failed to parse sync benchmark results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### âš¡ Async Benchmarks (AsyncNanaSQLite)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        python3 scripts/parse_benchmark.py async-benchmark-results async >> $GITHUB_STEP_SUMMARY || echo "Failed to parse async benchmark results" >> $GITHUB_STEP_SUMMARY

    # --- Performance Regression Detection (Only on main branch) ---
    - name: Merge all results for comparison
      if: github.ref == 'refs/heads/main'
      run: |
        python3 -c "
        import json
        import os
        from pathlib import Path

        combined = {'benchmarks': []}

        # Sync results
        for p in Path('benchmark-results').rglob('benchmark.json'):
            try:
                data = json.load(open(p))
                combined['benchmarks'].extend(data.get('benchmarks', []))
            except: pass

        # Async results
        for p in Path('async-benchmark-results').rglob('async-benchmark.json'):
            try:
                data = json.load(open(p))
                combined['benchmarks'].extend(data.get('benchmarks', []))
            except: pass

        # Use first found benchmark's machine info
        for p in Path('benchmark-results').rglob('benchmark.json'):
            try:
                data = json.load(open(p))
                if 'machine_info' in data:
                    combined['machine_info'] = data['machine_info']
                    combined['commit_info'] = data.get('commit_info', {})
                    combined['datetime'] = data.get('datetime', '')
                    break
            except: pass

        json.dump(combined, open('combined-benchmark.json', 'w'))
        print(f'Combined {len(combined[\"benchmarks\"])} benchmark results')
        "

    - name: Store and Compare Benchmark Result
      if: github.ref == 'refs/heads/main'
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: NanaSQLite Performance
        tool: 'pytest'
        output-file-path: combined-benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        # 120% = 20% slowdown threshold
        alert-threshold: '120%'
        comment-on-alert: true
        fail-on-alert: false
        gh-pages-branch: 'gh-pages'
        benchmark-data-dir-path: 'dev/bench'
