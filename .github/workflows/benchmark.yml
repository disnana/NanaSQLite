name: NanaSQLite Benchmarks

on:
  push:
    branches:
      - main
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: write
  deployments: write

jobs:
  # ============================================
  # Should-Run Check - Determines if benchmarks needed
  # ============================================
  should-run:
    name: Check if Benchmark needed
    runs-on: ubuntu-latest
    outputs:
      run_benchmark: ${{ steps.check.outputs.run_benchmark }}
    steps:
    - uses: actions/checkout@v6
      with:
        fetch-depth: 0

    - name: Check changed files
      id: check
      run: |
        # For workflow_dispatch: always run
        if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
          echo "run_benchmark=true" >> $GITHUB_OUTPUT
          echo "workflow_dispatch - Benchmark required"
          exit 0
        fi

        # For main branch: always run
        if [ "${{ github.ref }}" == "refs/heads/main" ]; then
          echo "run_benchmark=true" >> $GITHUB_OUTPUT
          echo "Main branch - Benchmark required"
          exit 0
        fi

        # For PRs: check if src/ changed
        if [ "${{ github.event_name }}" == "pull_request" ]; then
          BASE_SHA="${{ github.event.pull_request.base.sha }}"
        else
          BASE_SHA="${{ github.event.before }}"
        fi

        CHANGED_FILES=$(git diff --name-only $BASE_SHA ${{ github.sha }} 2>/dev/null || echo "")
        
        # Check if src/ files changed
        if echo "$CHANGED_FILES" | grep -qE '^src/'; then
          echo "run_benchmark=true" >> $GITHUB_OUTPUT
          echo "src/ changed - Benchmark required"
        else
          echo "run_benchmark=false" >> $GITHUB_OUTPUT
          echo "No src/ changes - Benchmark skipped"
        fi
  benchmark:
    needs: should-run
    if: needs.should-run.outputs.run_benchmark == 'true'
    runs-on: ${{ matrix.os }}
    timeout-minutes: 15
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.11", "3.13"]
        exclude:
          # Windows: only 3.13
          - os: windows-latest
            python-version: "3.11"

    steps:
    - uses: actions/checkout@v6
    
    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Setup Windows temp directory
      if: runner.os == 'Windows'
      shell: powershell
      run: New-Item -ItemType Directory -Force -Path D:\temp

    - name: Install dependencies
      run: |
        uv pip install --system pytest pytest-benchmark pytest-asyncio apsw pydantic cryptography orjson
        uv pip install --system -e .
    
    - name: Run sync benchmarks (Windows)
      if: runner.os == 'Windows'
      env:
        TEMP: 'D:\temp'
        TMP: 'D:\temp'
      run: |
        pytest tests/test_benchmark.py -v --benchmark-only --benchmark-json=benchmark.json || true

    - name: Run sync benchmarks (Linux/macOS)
      if: runner.os != 'Windows'
      run: |
        pytest tests/test_benchmark.py -v --benchmark-only --benchmark-json=benchmark.json || true
    
    - name: Upload sync benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: benchmark.json
        retention-days: 7

  async-benchmark:
    needs: should-run
    if: needs.should-run.outputs.run_benchmark == 'true'
    runs-on: ${{ matrix.os }}
    timeout-minutes: 15
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.11", "3.13"]
        exclude:
          # Windows: only 3.13
          - os: windows-latest
            python-version: "3.11"

    steps:
    - uses: actions/checkout@v6
    
    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Setup Windows temp directory
      if: runner.os == 'Windows'
      shell: powershell
      run: New-Item -ItemType Directory -Force -Path D:\temp

    - name: Install dependencies
      run: |
        uv pip install --system pytest pytest-benchmark pytest-asyncio apsw pydantic cryptography orjson
        uv pip install --system -e .
    
    - name: Run async benchmarks (Windows)
      if: runner.os == 'Windows'
      env:
        TEMP: 'D:\temp'
        TMP: 'D:\temp'
      run: |
        pytest tests/test_async_benchmark.py -v --benchmark-only --benchmark-json=async-benchmark.json || true

    - name: Run async benchmarks (Linux/macOS)
      if: runner.os != 'Windows'
      run: |
        pytest tests/test_async_benchmark.py -v --benchmark-only --benchmark-json=async-benchmark.json || true
    
    - name: Upload async benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: async-benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: async-benchmark.json
        retention-days: 7


  benchmark-summary:
    needs: [should-run, benchmark, async-benchmark]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    if: always()
    
    steps:
    - name: Skip Summary (No src changes)
      if: needs.should-run.outputs.run_benchmark != 'true'
      run: |
        echo "## ðŸ“Š Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "â­ï¸ **Benchmarks skipped** - No changes in \`src/\` directory" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Benchmarks only run when source code is modified." >> $GITHUB_STEP_SUMMARY

    - uses: actions/checkout@v6
      if: needs.should-run.outputs.run_benchmark == 'true'
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Fetch gh-pages branch for comparison
      if: needs.should-run.outputs.run_benchmark == 'true'
      run: |
        git fetch origin gh-pages:gh-pages || echo "gh-pages branch not found (first run)"
    
    - name: Download sync benchmark results
      if: needs.should-run.outputs.run_benchmark == 'true'
      uses: actions/download-artifact@v7
      with:
        pattern: benchmark-results-*
        path: benchmark-results
    
    - name: Download async benchmark results
      if: needs.should-run.outputs.run_benchmark == 'true'
      uses: actions/download-artifact@v7
      with:
        pattern: async-benchmark-results-*
        path: async-benchmark-results

    
    - name: Set up Python
      if: needs.should-run.outputs.run_benchmark == 'true'
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'
    
    - name: Generate Combined Benchmark Summary
      id: benchmark-results
      if: needs.should-run.outputs.run_benchmark == 'true'
      run: |
        echo "## ðŸ“Š Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "> âš ï¸ **Note**: GitHub Actions runners have performance variance. These results are informational only." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ”„ Sync Benchmarks (NanaSQLite)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        python3 scripts/parse_benchmark.py benchmark-results sync >> $GITHUB_STEP_SUMMARY || echo "Failed to parse sync benchmark results" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### âš¡ Async Benchmarks (AsyncNanaSQLite)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        python3 scripts/parse_benchmark.py async-benchmark-results async >> $GITHUB_STEP_SUMMARY || echo "Failed to parse async benchmark results" >> $GITHUB_STEP_SUMMARY


    # --- Performance Regression Detection (Only on main branch when benchmarks ran) ---
    - name: Merge all results for comparison
      if: github.ref == 'refs/heads/main' && needs.should-run.outputs.run_benchmark == 'true'
      run: |
        python3 -c "
        import json
        import os
        from pathlib import Path

        combined = {'benchmarks': []}

        # Sync results
        for p in Path('benchmark-results').rglob('benchmark.json'):
            try:
                data = json.load(open(p))
                combined['benchmarks'].extend(data.get('benchmarks', []))
            except: pass

        # Async results
        for p in Path('async-benchmark-results').rglob('async-benchmark.json'):
            try:
                data = json.load(open(p))
                combined['benchmarks'].extend(data.get('benchmarks', []))
            except: pass


        # Use first found benchmark's machine info
        for p in Path('benchmark-results').rglob('benchmark.json'):
            try:
                data = json.load(open(p))
                if 'machine_info' in data:
                    combined['machine_info'] = data['machine_info']
                    combined['commit_info'] = data.get('commit_info', {})
                    combined['datetime'] = data.get('datetime', '')
                    break
            except: pass

        json.dump(combined, open('combined-benchmark.json', 'w'))
        print(f'Combined {len(combined[\"benchmarks\"])} benchmark results')
        "

    - name: Store and Compare Benchmark Result
      if: github.ref == 'refs/heads/main' && needs.should-run.outputs.run_benchmark == 'true'
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: NanaSQLite Performance
        tool: 'pytest'
        output-file-path: combined-benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        # 120% = 20% slowdown threshold
        alert-threshold: '120%'
        comment-on-alert: true
        fail-on-alert: false
        gh-pages-branch: 'gh-pages'
        benchmark-data-dir-path: 'dev/bench'
