name: NanaSQLite Benchmarks

on:
  push:
    branches:
      - main
    paths:
      - 'src/**'
      - 'scripts/parse_benchmark.py'
      - '.github/workflows/benchmark.yml'
  pull_request:
    paths:
      - 'src/**'
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: write
  deployments: write

env:
  RETENTION_DAYS: 7

jobs:
  # ============================================
  # Benchmark Job (Sync & Async)
  # ============================================
  benchmark:
    name: Bench (${{ matrix.os }}, py${{ matrix.python-version }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.11", "3.13"]
        exclude:
          - os: windows-latest
            python-version: "3.11"

    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Setup Windows temp directory
      if: runner.os == 'Windows'
      shell: powershell
      run: New-Item -ItemType Directory -Force -Path D:\temp

    - name: Install dependencies
      run: |
        uv pip install --system pytest pytest-benchmark pytest-asyncio apsw pydantic
        uv pip install --system -e .
    
    # --- Sync Benchmarks with Retry ---
    - name: Run sync benchmarks (Retry Loop)
      shell: bash
      env:
        TEMP: ${{ runner.os == 'Windows' && 'D:\temp' || '/tmp' }}
        TMP: ${{ runner.os == 'Windows' && 'D:\temp' || '/tmp' }}
      run: |
        for i in {1..3}; do
          echo "::: Attempt $i for Sync Benchmarks :::"
          if pytest tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark.json; then
            echo "Sync benchmarks passed."
            break
          fi
          if [ $i -eq 3 ]; then
            echo "Error: Sync benchmarks failed after 3 attempts."
            exit 1
          fi
          echo "Attempt $i failed. Retrying..."
        done

    # --- Async Benchmarks with Retry ---
    - name: Run async benchmarks (Retry Loop)
      shell: bash
      env:
        TEMP: ${{ runner.os == 'Windows' && 'D:\temp' || '/tmp' }}
        TMP: ${{ runner.os == 'Windows' && 'D:\temp' || '/tmp' }}
      run: |
        for i in {1..3}; do
          echo "::: Attempt $i for Async Benchmarks :::"
          if pytest tests/test_async_benchmark.py --benchmark-only --benchmark-json=async-benchmark.json; then
            echo "Async benchmarks passed."
            break
          fi
          if [ $i -eq 3 ]; then
            echo "Error: Async benchmarks failed after 3 attempts."
            exit 1
          fi
          echo "Attempt $i failed. Retrying..."
        done
    
    # --- Merge results for github-action-benchmark ---
    - name: Merge Benchmark Results
      run: |
        python3 -c "
        import json
        sync = json.load(open('benchmark.json'))
        async_bench = json.load(open('async-benchmark.json'))
        sync['benchmarks'].extend(async_bench['benchmarks'])
        json.dump(sync, open('combined-benchmark.json', 'w'))
        "

    # --- Performance Regression Detection (Only on Ubuntu for consistency) ---
    - name: Store and Compare Benchmark Result
      if: runner.os == 'ubuntu-latest' && github.ref == 'refs/heads/main'
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: NanaSQLite Comparison (Ubuntu)
        tool: 'pytest'
        output-file-path: combined-benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        # 120% = 20% slow down threshold
        alert-threshold: '120%'
        comment-on-alert: true
        fail-on-alert: false # First time set to false to populate data
        gh-pages-branch: 'gh-pages'
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: |
          benchmark.json
          async-benchmark.json
        retention-days: ${{ env.RETENTION_DAYS }}

  # ============================================
  # Benchmark Summary - PR visibility
  # ============================================
  benchmark-summary:
    needs: [benchmark]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all results
      uses: actions/download-artifact@v4
      with:
        pattern: benchmark-results-*
        path: results
    
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'
    
    - name: Generate Summary
      run: |
        echo "## ðŸ“Š Performance Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # æ—¢å­˜ã®ãƒ‘ãƒ¼ã‚¹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åˆ©ç”¨ï¼ˆå„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«jsonãŒæ•£ã‚‰ã°ã£ã¦ã„ã‚‹ã®ã§èª¿æ•´ï¼‰
        for dir in results/*; do
          platform=$(basename $dir)
          echo "### Platform: $platform" >> $GITHUB_STEP_SUMMARY
          # scripts/parse_benchmark.py ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«ä¾å­˜ã—ã¦ã„ã‚‹ãŸã‚
          # ç°¡æ˜“çš„ã«å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã”ã¨ã«å®Ÿè¡Œã™ã‚‹ã‹ã€ä¸€æ™‚çš„ã«ãƒ•ãƒ©ãƒƒãƒˆã«ã™ã‚‹ãªã©ã®å·¥å¤«ãŒå¿…è¦
          # ã“ã“ã§ã¯æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒ results-dir ã‚’å¼•æ•°ã«å–ã‚‹ã®ã§ãã‚Œã‚’åˆ©ç”¨
        done
        
        python3 scripts/parse_benchmark.py results sync >> $GITHUB_STEP_SUMMARY || true
        python3 scripts/parse_benchmark.py results async >> $GITHUB_STEP_SUMMARY || true
