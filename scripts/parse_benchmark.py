#!/usr/bin/env python3
"""Parse multiple benchmark.json files and output improved markdown summary.

New format:
- Top 10 fastest/slowest operations
- Category-based grouping with collapsible sections
- OS comparison in horizontal format
- Performance diff vs previous run (when available)

Environment Variables:
    BENCHMARK_REGRESSION_THRESHOLD: Percentage threshold for severe regression (default: 20)
                                    If any test is slower than this threshold, exit with code 2.
"""
import json
import os
import subprocess
import sys
from collections import defaultdict
from pathlib import Path


def parse_single_benchmark(filepath):
    """1„Å§„ÅÆbenchmark.json„Çí„Éë„Éº„Çπ"""
    with open(filepath, encoding='utf-8') as f:
        return json.load(f)


def load_previous_benchmark():
    """Load previous benchmark data from gh-pages branch if available."""
    try:
        # Try to get the benchmark data from gh-pages branch
        result = subprocess.run(
            ['git', 'show', 'gh-pages:dev/bench/data.js'],
            capture_output=True,
            text=True,
            timeout=10
        )
        if result.returncode != 0:
            return {}
        
        # Parse the JS file (format: window.BENCHMARK_DATA = {...})
        content = result.stdout
        if 'window.BENCHMARK_DATA' not in content:
            return {}
        
        # Extract JSON part
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        if json_start == -1 or json_end == 0:
            return {}
        
        data = json.loads(content[json_start:json_end])
        
        # Get the most recent benchmark entry
        previous = {}
        for entry_name, entries in data.get('entries', {}).items():
            if entries:
                # Get the latest entry
                latest = entries[-1]
                for bench in latest.get('benches', []):
                    name = bench.get('name', '')
                    if '::' in name:
                        name = name.split('::')[-1]
                    if name:
                        # Store ops/sec directly for comparison
                        iter_per_sec = bench.get('value', 0)
                        previous[name] = iter_per_sec  # Keep as ops/sec
        
        return previous
    except Exception:
        return {}


def format_time(ms):
    """ÊôÇÈñì„ÇíÈÅ©Âàá„Å™Âçò‰Ωç„Åß„Éï„Ç©„Éº„Éû„ÉÉ„Éà"""
    if ms < 0.001:
        # 1 ms = 1,000,000 ns, so ms * 1,000,000 = ns
        return f"{ms * 1000000:.2f}ns"
    elif ms < 1:
        # 1 ms = 1,000 ¬µs, so ms * 1,000 = ¬µs
        return f"{ms * 1000:.2f}¬µs"
    elif ms < 1000:
        return f"{ms:.3f}ms"
    else:
        return f"{ms / 1000:.2f}s"


def format_ops(ms):
    """1Áßí„ÅÇ„Åü„Çä„ÅÆÊìç‰ΩúÂõûÊï∞(Ops/sec)„Çí„Éï„Ç©„Éº„Éû„ÉÉ„Éà"""
    if ms <= 0:
        return "0"
    ops = 1000.0 / ms
    if ops >= 1000000:
        return f"{ops / 1000000:.2f}M"
    elif ops >= 1000:
        return f"{ops / 1000:.1f}k"
    else:
        return f"{ops:.1f}"


def format_diff_ops(current_ops, previous_ops):
    """Format performance difference based on ops/sec.
    
    Higher ops/sec = better performance.
    Returns tuple of (formatted_string, improvement_percentage)
    
    Display convention:
      +X% = X% faster (improvement, more ops/sec)
      -X% = X% slower (regression, fewer ops/sec)
    """
    if previous_ops is None or previous_ops <= 0:
        return "-", 0
    
    if current_ops <= 0:
        return "üî¥ N/A", -100
    
    # Calculate percentage change in ops/sec
    # Positive = faster (more ops), Negative = slower (fewer ops)
    change_pct = ((current_ops / previous_ops) - 1) * 100
    
    # Determine emoji based on performance change
    if change_pct >= 5:
        emoji = "üöÄ"  # Significant improvement (5%+ faster)
    elif change_pct >= 1:
        emoji = "‚úÖ"  # Minor improvement
    elif change_pct > -1:
        emoji = "‚ûñ"  # No change
    elif change_pct > -5:
        emoji = "‚ö†Ô∏è"  # Minor regression
    else:
        emoji = "üî¥"  # Significant regression (5%+ slower)
    
    # Format: positive = faster (good), negative = slower (bad)
    if change_pct >= 0:
        sign = "+"
    else:
        sign = ""
    
    return f"{emoji} {sign}{change_pct:.1f}%", change_pct


def categorize_test(test_name):
    """„ÉÜ„Çπ„ÉàÂêç„Åã„Çâ„Ç´„ÉÜ„Ç¥„É™„ÇíÂà§ÂÆö"""
    name_lower = test_name.lower()
    
    if 'write' in name_lower or 'insert' in name_lower or 'set' in name_lower:
        return "‚úçÔ∏è Write Operations"
    elif 'read' in name_lower or 'get' in name_lower or 'fetch' in name_lower or 'load' in name_lower:
        return "üìñ Read Operations"
    elif 'batch' in name_lower:
        return "üì¶ Batch Operations"
    elif 'concurrent' in name_lower or 'mixed' in name_lower:
        return "üîÑ Concurrency"
    elif 'query' in name_lower or 'sql' in name_lower or 'execute' in name_lower:
        return "üóÉÔ∏è SQL Operations"
    elif 'table' in name_lower or 'index' in name_lower or 'schema' in name_lower or 'drop' in name_lower or 'create' in name_lower:
        return "üèóÔ∏è Schema Operations"
    elif 'pydantic' in name_lower or 'model' in name_lower:
        return "üî∑ Pydantic Operations"
    elif 'dict' in name_lower or 'keys' in name_lower or 'values' in name_lower or 'items' in name_lower or 'contains' in name_lower or 'len' in name_lower or 'pop' in name_lower or 'setdefault' in name_lower or 'to_dict' in name_lower:
        return "üìã Dict Operations"
    elif 'vacuum' in name_lower or 'refresh' in name_lower or 'pragma' in name_lower or 'copy' in name_lower or 'clear' in name_lower or 'update' in name_lower:
        return "üîß Utility Operations"
    elif 'transaction' in name_lower or 'commit' in name_lower or 'rollback' in name_lower:
        return "üíæ Transaction Operations"
    else:
        return "üìä Other Operations"


def get_os_emoji(os_name):
    """OSÂêç„Åã„ÇâÁµµÊñáÂ≠ó„ÇíÂèñÂæó"""
    if 'ubuntu' in os_name.lower():
        return 'üêß'
    elif 'windows' in os_name.lower():
        return 'ü™ü'
    elif 'macos' in os_name.lower():
        return 'üçé'
    else:
        return 'üíª'


def main():
    results_dir = sys.argv[1] if len(sys.argv) > 1 else '.'
    benchmark_type = sys.argv[2] if len(sys.argv) > 2 else 'sync'  # 'sync' or 'async'
    results_path = Path(results_dir)
    
    # Load previous benchmark data for comparison
    previous_data = load_previous_benchmark()
    
    # ÂÖ®„Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú„ÇíÂèéÈõÜ
    all_results = {}
    
    # „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çø„Éº„É≥
    if benchmark_type == 'async':
        dir_pattern = 'async-benchmark-results-'
        file_name = 'async-benchmark.json'
    else:
        dir_pattern = 'benchmark-results-'
        file_name = 'benchmark.json'
    
    # „Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖ„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú„ÇíÊé¢„Åô
    if results_path.is_dir():
        for subdir in results_path.iterdir():
            if subdir.is_dir() and subdir.name.startswith(dir_pattern):
                benchmark_file = subdir / file_name
                if benchmark_file.exists():
                    platform = subdir.name.replace(dir_pattern, '')
                    try:
                        all_results[platform] = parse_single_benchmark(benchmark_file)
                    except json.JSONDecodeError:
                        print(f"Warning: Failed to parse {benchmark_file}", file=sys.stderr)
    
    if not all_results:
        print("No benchmark results found.")
        return
    
    # ÂÖ®„ÉÜ„Çπ„ÉàÁµêÊûú„ÇíÈõÜË®à
    test_data = defaultdict(lambda: {'by_os': defaultdict(list), 'all_means': []})
    os_set = set()
    
    for platform, data in all_results.items():
        parts = platform.rsplit('-py', 1)
        if len(parts) == 2:
            os_name = parts[0]
            py_version = parts[1]
        else:
            os_name = platform
            py_version = 'unknown'
        
        os_set.add(os_name)
        
        for b in data.get('benchmarks', []):
            name = b['name'].split('::')[-1]
            mean = b['stats']['mean'] * 1000  # ms
            
            test_data[name]['by_os'][os_name].append({
                'py': py_version,
                'mean': mean
            })
            test_data[name]['all_means'].append({
                'os': os_name,
                'py': py_version,
                'mean': mean
            })
    
    if not test_data:
        print("No benchmark data to display.")
        return
    
    # OSÂêç„Çí„ÇΩ„Éº„Éà
    sorted_os = sorted(os_set)
    
    # ========== Top 10 Fastest / Slowest ==========
    # ÂêÑ„ÉÜ„Çπ„Éà„ÅÆÂπ≥ÂùáÊôÇÈñì„ÇíË®àÁÆó
    test_averages = []
    for test_name, data in test_data.items():
        means = [x['mean'] for x in data['all_means']]
        avg = sum(means) / len(means) if means else 0
        fastest = min(data['all_means'], key=lambda x: x['mean']) if data['all_means'] else None
        test_averages.append({
            'name': test_name,
            'avg': avg,
            'fastest': fastest
        })
    
    # Top 10 Fastest
    sorted_by_speed = sorted(test_averages, key=lambda x: x['avg'])
    print("#### üèÜ Top 10 Fastest Operations\n")
    print("| Rank | Test | Avg Time | Ops/sec | vs Prev | Fastest Platform |")
    print("|------|------|----------|---------|---------|------------------|")
    
    for i, item in enumerate(sorted_by_speed[:10], 1):
        fastest_str = f"{item['fastest']['os']} py{item['fastest']['py']}" if item['fastest'] else "-"
        prev_ops = previous_data.get(item['name'])
        current_ops = 1000.0 / item['avg'] if item['avg'] > 0 else 0
        diff_str, _ = format_diff_ops(current_ops, prev_ops)
        print(f"| {i} | {item['name']} | {format_time(item['avg'])} | {format_ops(item['avg'])} | {diff_str} | {fastest_str} |")
    
    print()
    
    # Top 10 Slowest
    sorted_by_slow = sorted(test_averages, key=lambda x: x['avg'], reverse=True)
    print("#### üê¢ Top 10 Slowest Operations\n")
    print("| Rank | Test | Avg Time | Ops/sec | vs Prev | Slowest Platform |")
    print("|------|------|----------|---------|---------|------------------|")
    
    for i, item in enumerate(sorted_by_slow[:10], 1):
        slowest = max(test_data[item['name']]['all_means'], key=lambda x: x['mean']) if test_data[item['name']]['all_means'] else None
        slowest_str = f"{slowest['os']} py{slowest['py']}" if slowest else "-"
        prev_ops = previous_data.get(item['name'])
        current_ops = 1000.0 / item['avg'] if item['avg'] > 0 else 0
        diff_str, _ = format_diff_ops(current_ops, prev_ops)
        print(f"| {i} | {item['name']} | {format_time(item['avg'])} | {format_ops(item['avg'])} | {diff_str} | {slowest_str} |")
    
    print()
    
    # ========== Category-based Grouping ==========
    categories = defaultdict(list)
    for test_name, data in test_data.items():
        category = categorize_test(test_name)
        means = [x['mean'] for x in data['all_means']]
        avg = sum(means) / len(means) if means else 0
        
        # OSÂà•„ÅÆÂπ≥ÂùáÂÄ§„ÇíË®àÁÆó
        os_avgs = {}
        for os_name in sorted_os:
            os_means = [x['mean'] for x in data['by_os'].get(os_name, [])]
            os_avgs[os_name] = sum(os_means) / len(os_means) if os_means else None
        
        categories[category].append({
            'name': test_name,
            'avg': avg,
            'os_avgs': os_avgs
        })
    
    print("#### üìã Results by Category\n")
    
    # „Ç´„ÉÜ„Ç¥„É™„Åî„Å®„Å´Âá∫Âäõ
    for category in sorted(categories.keys()):
        tests = categories[category]
        tests_sorted = sorted(tests, key=lambda x: x['avg'])
        
        # „Éò„ÉÉ„ÉÄ„Éº‰ΩúÊàê
        os_headers = " | ".join([f"{get_os_emoji(os)} {os.replace('-latest', '')}" for os in sorted_os])
        
        print(f"<details><summary>{category} ({len(tests)} tests)</summary>\n")
        print(f"| Test | Avg | Ops/sec | vs Prev | {os_headers} |")
        print(f"|------|-----|---------|---------|" + "|".join(["-----"] * len(sorted_os)) + "|")
        
        for test in tests_sorted:
            os_values = []
            for os_name in sorted_os:
                val = test['os_avgs'].get(os_name)
                if val:
                    os_values.append(f"{format_time(val)}<br>({format_ops(val)})")
                else:
                    os_values.append("-")
            
            os_cells = " | ".join(os_values)
            prev_ops = previous_data.get(test['name'])
            current_ops = 1000.0 / test['avg'] if test['avg'] > 0 else 0
            diff_str, _ = format_diff_ops(current_ops, prev_ops)
            print(f"| {test['name']} | {format_time(test['avg'])} | {format_ops(test['avg'])} | {diff_str} | {os_cells} |")
        
        print("\n</details>\n")
    
    # ========== Summary Stats ==========
    has_significant_regression = False
    if test_averages:
        improvements = 0
        regressions = 0
        severe_regressions = 0  # >20% slower
        unchanged = 0
        no_data = 0
        
        regression_threshold = float(os.environ.get('BENCHMARK_REGRESSION_THRESHOLD', '20'))
        
        for item in test_averages:
            prev_ops = previous_data.get(item['name'])
            if prev_ops and prev_ops > 0 and item['avg'] > 0:
                current_ops = 1000.0 / item['avg']
                # change_pct: positive = faster (improvement), negative = slower (regression)
                change_pct = ((current_ops / prev_ops) - 1) * 100
                
                if change_pct >= 1:
                    improvements += 1
                elif change_pct <= -regression_threshold:
                    severe_regressions += 1
                    regressions += 1
                elif change_pct <= -1:
                    regressions += 1
                else:
                    unchanged += 1
            else:
                no_data += 1
        
        total_compared = improvements + regressions + unchanged
        if total_compared > 0:
            print("#### üìà Performance Summary vs Previous\n")
            print(f"- üöÄ **Improved**: {improvements} tests")
            print(f"- ‚ûñ **Unchanged**: {unchanged} tests")
            print(f"- ‚ö†Ô∏è **Regressed**: {regressions} tests")
            if severe_regressions > 0:
                print(f"- üî¥ **Severe (>{regression_threshold:.0f}%)**: {severe_regressions} tests")
                has_significant_regression = True
            if no_data > 0:
                print(f"- ‚ùì **No previous data**: {no_data} tests")
            print()
    
    # Return exit code 2 if significant regression detected
    # (exit code 1 is reserved for errors)
    return 2 if has_significant_regression else 0


if __name__ == '__main__':
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        print(f"Error parsing benchmark results: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)
