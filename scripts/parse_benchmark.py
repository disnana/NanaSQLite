#!/usr/bin/env python3
"""Parse multiple benchmark.json files and output improved markdown summary.

New format:
- Top 10 fastest/slowest operations
- Category-based grouping with collapsible sections
- OS comparison in horizontal format
- Performance diff vs previous run (when available)

Environment Variables:
    BENCHMARK_REGRESSION_THRESHOLD: Percentage threshold for severe regression (default: 20)
                                    If any test is slower than this threshold, exit with code 2.
"""
import json
import os
import subprocess
import sys
from collections import defaultdict
from pathlib import Path


def parse_single_benchmark(filepath):
    """1ã¤ã®benchmark.jsonã‚’ãƒ‘ãƒ¼ã‚¹"""
    with open(filepath, encoding='utf-8') as f:
        return json.load(f)


def load_previous_benchmark():
    """Load previous benchmark data from gh-pages branch if available."""
    try:
        # Try to get the benchmark data from gh-pages branch
        result = subprocess.run(
            ['git', 'show', 'gh-pages:dev/bench/data.js'],
            capture_output=True,
            text=True,
            timeout=10
        )
        if result.returncode != 0:
            return {}
        
        # Parse the JS file (format: window.BENCHMARK_DATA = {...})
        content = result.stdout
        if 'window.BENCHMARK_DATA' not in content:
            return {}
        
        # Extract JSON part
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        if json_start == -1 or json_end == 0:
            return {}
        
        data = json.loads(content[json_start:json_end])
        
        # Get the most recent benchmark entry
        previous = {}
        for entry_name, entries in data.get('entries', {}).items():
            if entries:
                # Get the latest entry
                latest = entries[-1]
                for bench in latest.get('benches', []):
                    name = bench.get('name', '')
                    if '::' in name:
                        name = name.split('::')[-1]
                    if name:
                        # Store ops/sec directly for comparison
                        iter_per_sec = bench.get('value', 0)
                        previous[name] = iter_per_sec  # Keep as ops/sec
        
        return previous
    except Exception:
        return {}


def format_time(ms):
    """æ™‚é–“ã‚’é©åˆ‡ãªå˜ä½ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if ms < 0.001:
        # 1 ms = 1,000,000 ns, so ms * 1,000,000 = ns
        return f"{ms * 1000000:.2f}ns"
    elif ms < 1:
        # 1 ms = 1,000 Âµs, so ms * 1,000 = Âµs
        return f"{ms * 1000:.2f}Âµs"
    elif ms < 1000:
        return f"{ms:.3f}ms"
    else:
        return f"{ms / 1000:.2f}s"


def format_ops(ops):
    """1ç§’ã‚ãŸã‚Šã®æ“ä½œå›æ•°(Ops/sec)ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if ops <= 0:
        return "0"
    if ops >= 1000000:
        return f"{ops / 1000000:.2f}M"
    elif ops >= 1000:
        return f"{ops / 1000:.1f}k"
    else:
        return f"{ops:.1f}"


def format_diff_ops(current_ops, previous_ops):
    """Format performance difference based on ops/sec.
    
    Higher ops/sec = better performance.
    Returns tuple of (formatted_string, improvement_percentage)
    
    Display convention:
      +X% (+Y ops) = X% faster (improvement, more ops/sec)
      -X% (-Y ops) = X% slower (regression, fewer ops/sec)
    """
    if previous_ops is None or previous_ops <= 0:
        return "-", 0
    
    if current_ops <= 0:
        return "ğŸ”´ N/A", -100
    
    # Calculate percentage change in ops/sec
    # Positive = faster (more ops), Negative = slower (fewer ops)
    change_pct = ((current_ops / previous_ops) - 1) * 100
    ops_diff = current_ops - previous_ops
    
    # Determine emoji based on performance change
    if change_pct >= 5:
        emoji = "ğŸš€"  # Significant improvement (5%+ faster)
    elif change_pct >= 1:
        emoji = "âœ…"  # Minor improvement
    elif change_pct > -1:
        emoji = "â–"  # No change
    elif change_pct > -5:
        emoji = "âš ï¸"  # Minor regression
    else:
        emoji = "ğŸ”´"  # Significant regression (5%+ slower)
    
    # Format ops difference with appropriate unit
    def format_ops_diff(ops):
        abs_ops = abs(ops)
        if abs_ops >= 1000000:
            return f"{ops/1000000:+.1f}M"
        elif abs_ops >= 1000:
            return f"{ops/1000:+.1f}k"
        else:
            return f"{ops:+.0f}"
    
    # Format: positive = faster (good), negative = slower (bad)
    if change_pct >= 0:
        sign = "+"
    else:
        sign = ""
    
    return f"{emoji} {sign}{change_pct:.1f}% ({format_ops_diff(ops_diff)})", change_pct


def categorize_test(test_name):
    """ãƒ†ã‚¹ãƒˆåã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®š"""
    name_lower = test_name.lower()
    
    if 'write' in name_lower or 'insert' in name_lower or 'set' in name_lower:
        return "âœï¸ Write Operations"
    elif 'read' in name_lower or 'get' in name_lower or 'fetch' in name_lower or 'load' in name_lower:
        return "ğŸ“– Read Operations"
    elif 'batch' in name_lower:
        return "ğŸ“¦ Batch Operations"
    elif 'concurrent' in name_lower or 'mixed' in name_lower:
        return "ğŸ”„ Concurrency"
    elif 'query' in name_lower or 'sql' in name_lower or 'execute' in name_lower:
        return "ğŸ—ƒï¸ SQL Operations"
    elif 'table' in name_lower or 'index' in name_lower or 'schema' in name_lower or 'drop' in name_lower or 'create' in name_lower:
        return "ğŸ—ï¸ Schema Operations"
    elif 'pydantic' in name_lower or 'model' in name_lower:
        return "ğŸ”· Pydantic Operations"
    elif 'dict' in name_lower or 'keys' in name_lower or 'values' in name_lower or 'items' in name_lower or 'contains' in name_lower or 'len' in name_lower or 'pop' in name_lower or 'setdefault' in name_lower or 'to_dict' in name_lower:
        return "ğŸ“‹ Dict Operations"
    elif 'vacuum' in name_lower or 'refresh' in name_lower or 'pragma' in name_lower or 'copy' in name_lower or 'clear' in name_lower or 'update' in name_lower:
        return "ğŸ”§ Utility Operations"
    elif 'transaction' in name_lower or 'commit' in name_lower or 'rollback' in name_lower:
        return "ğŸ’¾ Transaction Operations"
    else:
        return "ğŸ“Š Other Operations"


def get_os_emoji(os_name):
    """OSåã‹ã‚‰çµµæ–‡å­—ã‚’å–å¾—"""
    if 'ubuntu' in os_name.lower():
        return 'ğŸ§'
    elif 'windows' in os_name.lower():
        return 'ğŸªŸ'
    elif 'macos' in os_name.lower():
        return 'ğŸ'
    else:
        return 'ğŸ’»'


def main():
    results_dir = sys.argv[1] if len(sys.argv) > 1 else '.'
    benchmark_type = sys.argv[2] if len(sys.argv) > 2 else 'sync'  # 'sync' or 'async'
    results_path = Path(results_dir)
    
    # Load previous benchmark data for comparison
    previous_data = load_previous_benchmark()
    
    # å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’åé›†
    all_results = {}
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    if benchmark_type == 'async':
        dir_pattern = 'async-benchmark-results-'
        file_name = 'async-benchmark.json'
    else:
        dir_pattern = 'benchmark-results-'
        file_name = 'benchmark.json'
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’æ¢ã™
    if results_path.is_dir():
        for subdir in results_path.iterdir():
            if subdir.is_dir() and subdir.name.startswith(dir_pattern):
                benchmark_file = subdir / file_name
                if benchmark_file.exists():
                    platform = subdir.name.replace(dir_pattern, '')
                    try:
                        all_results[platform] = parse_single_benchmark(benchmark_file)
                    except json.JSONDecodeError:
                        print(f"Warning: Failed to parse {benchmark_file}", file=sys.stderr)
    
    if not all_results:
        print("No benchmark results found.")
        return
    
    # å…¨ãƒ†ã‚¹ãƒˆçµæœã‚’é›†è¨ˆ
    test_data = defaultdict(lambda: {'by_os': defaultdict(list), 'all_means': []})
    os_set = set()
    
    for platform, data in all_results.items():
        parts = platform.rsplit('-py', 1)
        if len(parts) == 2:
            os_name = parts[0]
            py_version = parts[1]
        else:
            os_name = platform
            py_version = 'unknown'
        
        os_set.add(os_name)
        
        for b in data.get('benchmarks', []):
            name = b['name'].split('::')[-1]
            mean = b['stats']['mean'] * 1000  # ms
            ops = b['stats'].get('ops', 0)    # raw ops/sec
            
            test_data[name]['by_os'][os_name].append({
                'py': py_version,
                'mean': mean,
                'ops': ops
            })
            test_data[name]['all_means'].append({
                'os': os_name,
                'py': py_version,
                'mean': mean,
                'ops': ops
            })
    
    if not test_data:
        print("No benchmark data to display.")
        return
    
    # OSåã‚’ã‚½ãƒ¼ãƒˆ
    sorted_os = sorted(os_set)
    
    # ========== Top 10 Fastest / Slowest ==========
    # å„ãƒ†ã‚¹ãƒˆã®å¹³å‡æ™‚é–“ã‚’è¨ˆç®—
    test_averages = []
    for test_name, data in test_data.items():
        means = [x['mean'] for x in data['all_means']]
        ops_list = [x['ops'] for x in data['all_means']]
        
        avg = sum(means) / len(means) if means else 0
        avg_ops = sum(ops_list) / len(ops_list) if ops_list else 0
        
        fastest = min(data['all_means'], key=lambda x: x['mean']) if data['all_means'] else None
        test_averages.append({
            'name': test_name,
            'avg': avg,
            'ops': avg_ops,
            'fastest': fastest
        })
    
    # Top 10 Fastest
    sorted_by_speed = sorted(test_averages, key=lambda x: x['avg'])
    print("#### ğŸ† Top 10 Fastest Operations\n")
    print("| Rank | Test | Avg Time | Ops/sec | vs Prev | Fastest Platform |")
    print("|------|------|----------|---------|---------|------------------|")
    
    for i, item in enumerate(sorted_by_speed[:10], 1):
        fastest_str = f"{item['fastest']['os']} py{item['fastest']['py']}" if item['fastest'] else "-"
        prev_ops = previous_data.get(item['name'])
        current_ops = item['ops']
        diff_str, _ = format_diff_ops(current_ops, prev_ops)
        print(f"| {i} | {item['name']} | {format_time(item['avg'])} | {format_ops(current_ops)} | {diff_str} | {fastest_str} |")
    
    print()
    
    # Top 10 Slowest
    sorted_by_slow = sorted(test_averages, key=lambda x: x['avg'], reverse=True)
    print("#### ğŸ¢ Top 10 Slowest Operations\n")
    print("| Rank | Test | Avg Time | Ops/sec | vs Prev | Slowest Platform |")
    print("|------|------|----------|---------|---------|------------------|")
    
    for i, item in enumerate(sorted_by_slow[:10], 1):
        slowest = max(test_data[item['name']]['all_means'], key=lambda x: x['mean']) if test_data[item['name']]['all_means'] else None
        slowest_str = f"{slowest['os']} py{slowest['py']}" if slowest else "-"
        prev_ops = previous_data.get(item['name'])
        current_ops = item['ops']
        diff_str, _ = format_diff_ops(current_ops, prev_ops)
        print(f"| {i} | {item['name']} | {format_time(item['avg'])} | {format_ops(current_ops)} | {diff_str} | {slowest_str} |")
    
    print()
    
    # ========== Category-based Grouping ==========
    categories = defaultdict(list)
    for test_name, data in test_data.items():
        category = categorize_test(test_name)
        means = [x['mean'] for x in data['all_means']]
        ops_list = [x['ops'] for x in data['all_means']]
        
        avg = sum(means) / len(means) if means else 0
        avg_ops = sum(ops_list) / len(ops_list) if ops_list else 0
        
        # OSåˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—
        os_stats = {}
        for os_name in sorted_os:
            entries = data['by_os'].get(os_name, [])
            if entries:
                mean_avg = sum(x['mean'] for x in entries) / len(entries)
                ops_avg = sum(x['ops'] for x in entries) / len(entries)
                os_stats[os_name] = {'mean': mean_avg, 'ops': ops_avg}
            else:
                os_stats[os_name] = None
        
        categories[category].append({
            'name': test_name,
            'avg': avg,
            'ops': avg_ops,
            'os_stats': os_stats
        })
    
    print("#### ğŸ“‹ Results by Category\n")
    
    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«å‡ºåŠ›
    for category in sorted(categories.keys()):
        tests = categories[category]
        tests_sorted = sorted(tests, key=lambda x: x['avg'])
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        os_headers = " | ".join([f"{get_os_emoji(os)} {os.replace('-latest', '')}" for os in sorted_os])
        
        print(f"<details><summary>{category} ({len(tests)} tests)</summary>\n")
        print(f"| Test | Avg | Ops/sec | vs Prev | {os_headers} |")
        print(f"|------|-----|---------|---------|" + "|".join(["-----"] * len(sorted_os)) + "|")
        
        for test in tests_sorted:
            os_values = []
            for os_name in sorted_os:
                stats = test['os_stats'].get(os_name)
                if stats:
                    os_values.append(f"{format_time(stats['mean'])}<br>({format_ops(stats['ops'])})")
                else:
                    os_values.append("-")
            
            os_cells = " | ".join(os_values)
            prev_ops = previous_data.get(test['name'])
            current_ops = test['ops']
            diff_str, _ = format_diff_ops(current_ops, prev_ops)
            print(f"| {test['name']} | {format_time(test['avg'])} | {format_ops(current_ops)} | {diff_str} | {os_cells} |")
        
        print("\n</details>\n")
    
    # ========== Summary Stats ==========
    has_significant_regression = False
    if test_averages:
        improvements = 0
        regressions = 0
        severe_regressions = 0  # >20% slower
        unchanged = 0
        no_data = 0
        
        regression_threshold = float(os.environ.get('BENCHMARK_REGRESSION_THRESHOLD', '20'))
        
        for item in test_averages:
            prev_ops = previous_data.get(item['name'])
            if prev_ops and prev_ops > 0 and item['avg'] > 0:
                current_ops = item['ops']
                # change_pct: positive = faster (improvement), negative = slower (regression)
                change_pct = ((current_ops / prev_ops) - 1) * 100
                
                if change_pct >= 1:
                    improvements += 1
                elif change_pct <= -regression_threshold:
                    severe_regressions += 1
                    regressions += 1
                elif change_pct <= -1:
                    regressions += 1
                else:
                    unchanged += 1
            else:
                no_data += 1
        
        total_compared = improvements + regressions + unchanged
        if total_compared > 0:
            print("#### ğŸ“ˆ Performance Summary vs Previous\n")
            print(f"- ğŸš€ **Improved**: {improvements} tests")
            print(f"- â– **Unchanged**: {unchanged} tests")
            print(f"- âš ï¸ **Regressed**: {regressions} tests")
            if severe_regressions > 0:
                print(f"- ğŸ”´ **Severe (>{regression_threshold:.0f}%)**: {severe_regressions} tests")
                has_significant_regression = True
            if no_data > 0:
                print(f"- â“ **No previous data**: {no_data} tests")
            print()
    
    # Return exit code 2 if significant regression detected
    # (exit code 1 is reserved for errors)
    return 2 if has_significant_regression else 0


if __name__ == '__main__':
    # Ensure UTF-8 output for Windows to handle emojis correctly
    if sys.stdout.encoding != 'utf-8':
        try:
            sys.stdout.reconfigure(encoding='utf-8')
        except AttributeError:
            # Python < 3.7 doesn't support reconfigure, but we target >= 3.9
            pass

    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        print(f"Error parsing benchmark results: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)
